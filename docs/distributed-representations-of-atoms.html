<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="The Materialis.AI research blog discusses the application of Machine Learning to Materials Science">
	<title>Materialis.AI - Blog</title>
    <link href="https://fonts.googleapis.com/css?family=Droid+Serif|Source+Sans+Pro" rel="stylesheet">
	<link rel="stylesheet" href="main.css">
    <link rel="icon" type="image/png" href="images/materialis-icon.png"/>
</head>

<body>

<div class="header">
    <a href="https://materialis.ai" class="homelink">
        <img src="images/materialisai-logo.png" width="175" class="logo" alt="logo">
    </a>
    <div class="header-right">
        <a href="https://materialis.ai/contact.html"><div>Contact</div></a>
    </div>
    <div class="header-right">
        <a href="https://materialis.ai/about.html"><div>About</div></a>
    </div>
    <div class="header-right">
        <a href="/"><div>Posts</div></a>
    </div>
</div>

<div id="container">

    <!-- begin content -->

    <H1>
        Distributed Representations of Atoms
    </H1>
    <span class="post-meta">Mar 18, 2022 &nbsp; &#8226; &nbsp; Luis M. Antunes</span>

    <div id="content">

        <p>
            An important aspect of applying Machine Learning (ML) to a problem involves choosing a suitable
            representation for the objects under consideration. For example, to classify a flower that belongs to one of
            a number of different species, one might choose to represent the flower as a list of scalar and categorical
            features, such as petal length, petal width, and color. The ability of an ML model to properly perform a
            task depends strongly on the quality of the representation used. In fact, this is so important in
            determining the success of an ML project, that feature engineering is often the activity that receives the
            most attention.
        </p>

        <p>
            For many types of problems that ML is applied to, the advent of
            <a href="https://www.nature.com/articles/nature14539" target="_blank">Deep Learning</a> changed things dramatically.
            Instead of requiring the manual engineering of features, Deep Learning is often capable of automatically
            building suitable representations from more basic, and often higher-dimensional representations of the
            objects under consideration. In Computer Vision and image processing, this basic representation typically
            consists of the pixels of the image. In Natural Language Processing (NLP), the basic representation is often
            a <a href="https://en.wikipedia.org/wiki/One-hot#Machine_learning_and_statistics" target="_blank">one-hot vector</a>
            (when the objects are individual words), or a
            <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" target="_blank">bag-of-words vector</a> (when the
            objects are sentences). In board games, such as Go or Chess, the representation can be layers of binary
            matrices that represent player and opponent pieces and their relative positions (amongst other elements of
            the game). In all of these examples, the basic representations are high-dimensional, and require relatively
            little effort from the ML practitioner to devise. Classic ML algorithms, such as the Support Vector Machine,
            or the Random Forest, would likely have problems learning with such high-dimensional representations. A Deep
            Learning model, on the other hand, is often able to automatically extract the representation best suited for
            the task at hand starting from such basic representations, since internal representations are learned
            end-to-end, by taking advantage of the error signal provided by backpropagation.
        </p>

        <H2>
            Local vs. Distributed Representations
        </H2>

        <p>
            The representations a Deep Learning model learns, however, may not be readily interpretable by a human. The
            kinds of representations learned by a Deep Learning model usually stand in contrast to a classical, manually
            constructed representation. Such manually curated feature vectors can be called <span class="emph">local
            representations</span>, since each of the elements in the list of features may have little to do with the
            other elements, and typically represents something concrete and intelligible to a human. Representations
            in which each of the elements are coupled in some way, and work together to represent the object under
            consideration, can be called <span class="emph">distributed representations</span>. A Deep Learning model
            usually learns distributed representations.
        </p>

        <figure>
            <div>
                <img src="images/local-distributed.png" alt="local vs. distributed representations" />
            </div>
            <figcaption>
                Figure 1: Illustration of one-hot and distributed representations. In the diagram, there are <i>n</i>
                kinds of objects represented, and <i>d</i> is the adjustable number of dimensions of the distributed
                representation.
            </figcaption>
        </figure>

        <p>
            But what do these distributed representations mean? How do they represent the object under consideration?
            Essentially, they convert the object they represent into a point in a multi- (usually lower-) dimensional
            space, in such a way that the relationship to other objects is preserved. This usually means that the
            Euclidean distance between two such objects/points reflects their similarity. These representations thus
            provide a more principled structure to the input data, and are usually lower-dimensional, which should allow
            an ML model to learn a task more quickly and effectively.
        </p>

        <H2>
            Word2vec
        </H2>

        <p>
            This is nothing terribly new in the world of ML. In fact, the power of distributed representations was
            perhaps initially demonstrated in the field of NLP. In NLP tasks, individual words are commonly represented
            by vectors. As mentioned above, these can be one-hot vectors, but they are often pre-trained distributed
            representations. Pre-training is a procedure that typically involves applying an unsupervised learning
            algorithm to a dataset. This has been one of the ways that distributed representations of objects, such as
            words, are created. Representations created this way can be re-used in downstream tasks. Using pre-trained
            representations has the effect of accelerating and even improving learning. One such unsupervised learning
            algorithm for developing pre-trained distributed representations of words is
            <a href="https://en.wikipedia.org/wiki/Word2vec" target="_blank">Word2vec</a>. The result of the algorithm
            is that a vector is assigned to each word in the vocabulary present in the dataset; each word is assigned a
            unique vector from a learned semantic space, extracted automatically from the structure of the data. And
            since the words inhabit a structured semantic space, they can be combined using logical operations (such
            as addition and subtraction) to produce new vectors which also inhabit the same space.
        </p>

        <p>
            How does the Word2vec algorithm work? Consider the adage <i>&quot;You shall know a word by the company
            it keeps.&quot;</i> (attributed to J. R. Firth). The algorithm takes this concept and formalizes it as a
            learning problem. Specifically, the objective is to produce a model that assigns maximal probability to a
            word when it occurs in the context of certain other words. In practice, this usually means the model must
            learn to predict the neighbouring words that most often appear with it. The idea is that a word gets its
            meaning from the context in which it is used. Consider the statement &quot;A person walks into a
            store&quot;. How probable is it that the word &quot;person&quot; appears in the context of the words
            &quot;walks&quot; and &quot;store&quot;? It's surely more probable than the word &quot;car&quot;
            appearing in that context, for example. As we'll see shortly, the model used to predict neighbouring words is a
            feed-forward network with a single hidden layer, and the initial parameter matrix that transforms the input
            into the hidden layer is where we'll find the distributed representations for each word.
        </p>

        <p>
            What does any of this have to do with atoms, or Materials Science? Consider that ML and
            Deep Learning are playing increasingly prominent roles in Computational Materials Science. It's not
            surprising, since many of the tasks encountered in Computational Materials Science involve the prediction
            of the properties of a material. The number of datasets related to materials properties is also increasing.
            It therefore seems natural that the advances made in ML should find application in Computational Materials
            Science. As such, a natural question to ask is: how are materials represented in the context of an ML task?
            The approaches being used are rapidly changing, but traditionally, materials and atoms are represented using
            local representations. For example, it is common to see atoms represented as vectors of features such as
            electronegativity, atomic radius, ionic radius, etc. If a specific class of material is being considered,
            such as the perovskites, with formula ABX<sub>3</sub>, then a material can, for example, be represented by
            concatenating the A, B and X atom vectors.
        </p>

        <H2>
            SkipAtom
        </H2>

        <p>
            In recent years, pre-trained distributed representations have begun making their way into Computational
            Materials Science as well. Two of the more widely known atomic representations of this kind are
            <a href="https://www.pnas.org/doi/10.1073/pnas.1801181115" target="_blank">Atom2Vec</a> and
            <a href="https://www.nature.com/articles/s41586-019-1335-8" target="_blank">Mat2Vec</a>. We won't go into
            how these algorithms work here, but in brief, Atom2Vec takes the compositions found in materials databases,
            constructs a large co-occurrence matrix of atoms and their atomic environments, and performs singular value
            decomposition to obtain atom vectors. Mat2Vec, on the other hand, applies the Word2vec algorithm to a large
            number of abstracts from the materials literature, resulting in representations being learned for atoms (in
            addition to other kinds of entities). In a
            <a href="https://www.nature.com/articles/s41524-022-00729-3" target="_blank">paper</a> published today in
            <i>npj Computational Materials</i>, we introduce another algorithm for building distributed representations
            for atoms, which we call <span class="emph">SkipAtom</span>.
        </p>

        <p>
            We introduced SkipAtom because we believe it has some advantages over the other existing approaches (which
            we discuss in the
            <a href="https://www.nature.com/articles/s41524-022-00729-3" target="_blank">paper</a>). The SkipAtom
            approach makes an analogy between words and atoms, and sentences and chemical compositions. It attempts to
            formalize the idea that an atom shall be known by &quot;the company it keeps&quot;. We expect to learn the
            &quot;chemical semantics&quot; of an atom by observing the chemo-structural context in which it occurs. This
            means that, analogously to Word2vec, we will build a model in an unsupervised fashion that assigns maximal
            probability to an atom when it occurs within the context of certain other atoms. In practice, the model
            learns to predict which atoms typically surround any given atom. We won't go into the details here, but we
            need to be precise about what we mean by &quot;surrounds&quot;, and in brief, this means applying existing
            algorithms that operate on a crystal structure to create a graph representing what's connected to what, and
            examining the immediate neighbours of a given atom in the graph.
        </p>

        <figure>
            <div>
                <img src="images/skipatom-training-pairs.png" alt="training pairs" />
            </div>
            <figcaption>
                Figure 2: Crystal structures in a materials database are converted into co-occurring atom pairs for
                training of SkipAtom vectors.
            </figcaption>
        </figure>

        <p>
            A crystal structure database is used for learning the SkipAtom vectors. We use the
            <a href="https://materialsproject.org/" target="_blank"> Materials Project</a> database, from which we
            obtain over 126,000 compounds and their crystal structures. The first step requires the creation of training
            pairs, as depicted in Figure 2. This results in over 15 million pairs being generated. These pairs of
            co-occurring atoms are then used in a prediction task: given the first atom in the pair, the model must
            predict what the other atom is. In practice, this means minimizing the cross-entropy loss between the actual
            paired atom and the predicted one, given that atoms are represented as one-hot vectors at this stage. The
            atom vectors are found in the embedding matrix, <i><b>W</b><sub>e</sub></i> (see Figure 3).
        </p>

        <figure>
            <div>
                <img src="images/skipatom-model-training.png" alt="model training" />
            </div>
            <figcaption>
                Figure 3: A depiction of how SkipAtom vectors are derived through training. An atom, represented as a
                one-hot vector, <i><b>x</b></i>, is multiplied with the matrix <i><b>W</b><sub>e</sub></i>, to yield the
                intermediate vector <i><b>h</b></i>. Then <i><b>h</b></i> is multiplied with
                <i><b>W</b><sub>s</sub></i>, to which a <i>softmax</i> operation is applied to obtain the output
                <i><b>y&#770;</b></i>. During training, the cross-entropy loss between the context atom, represented
                by <i><b>y</b></i>, and the predicted atom, <i><b>y&#770;</b></i>, is minimized. The columns of the
                matrix <i><b>W</b><sub>e</sub></i> will be the learned atom vectors, after training is complete.
            </figcaption>
        </figure>

        <H2>
            Results
        </H2>

        <p>
            A common way of visualizing the results of learning such representations is to apply a dimensionality
            reduction technique, such as
            <a href="https://en.wikipedia.org/wiki/Principal_component_analysis" target="_blank">PCA</a> or
            <a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding" target="_blank">t-SNE</a>.
            The vectors can be reduced to 2 or 3 dimensions, and then plotted. We applied t-SNE to the learned SkipAtom
            atom vectors, reducing them to 2 dimensions from their original 200 dimensions, and plotted them. See Figure
            4. There appears to be logical structure to the data. For example, the alkali metals are clustered together,
            as are the light non-metals. It is important to note that while the locations of the atoms in the plot may
            look somewhat arbitrary, they in fact reflect chemo-structural nuances gleaned from the dataset.
        </p>

        <figure>
            <div>
                <img src="images/skipatom-tsne.png" alt="tSNE" />
            </div>
            <figcaption>
                Figure 4: Dimensionally reduced SkipAtom atom vectors with an original size of 200 dimensions. The
                vectors were reduced to 2 dimensions using t-SNE.
            </figcaption>
        </figure>

        <p>
            Visualizing the representations helps us to understand something about the structure of the learned space,
            however, the best way to assess their quality is to use them in a task, and examine the resulting
            performance. For benchmarking and comparison purposes, there exists a collection of datasets and tasks for
            Computational Materials Science, known as the
            <a href="https://matbench.materialsproject.org/" target="_blank">Matbench</a> test suite. We took a number
            of tasks from this suite, and compared the performance using an
            <a href="https://www.nature.com/articles/s41598-018-35934-y" target="_blank">ElemNet</a> architecture with
            various kinds of atom vector representations. The results are depicted in Figure 5.
        </p>

        <figure>
            <div>
                <img src="images/skipatom-performance.png" alt="tSNE" />
            </div>
            <figcaption>
                Figure 5: A comparison of results on benchmark tasks. TBG refers to the Theoretical
                Band Gap task (MAE in eV), BM to the Bulk Modulus task (MAE in log(GPa)), SM to the Shear Modulus task
                (MAE in log(GPa)), RI to the Refractive Index task (MAE in n), and TM to the Theoretical Metallicity
                task (ROC-AUC). These tasks make use of structure information. EBG refers to the Experimental Band Gap
                task (MAE in eV), BMGF to the Bulk Metallic Glass Formation task (ROC-AUC), EM to the Experimental
                Metallicity task (ROC-AUC). These tasks make use of composition only. The results that are outlined in
                bold represent the best score for that task.
            </figcaption>
        </figure>

        <p>
            From the results above, we conclude that SkipAtom is about as effective as Mat2Vec, and better on some
            tasks. It also clearly performs better than Atom2Vec (not shown here). Its real advantages, however, are
            its conceptual simplicity, and accessibility to researchers. While Mat2Vec requires the curation of millions
            of scientific abstracts, and a number of hand-crafted processing rules, SkipAtom requires access to a
            dataset of crystal structure information, which can be of any size. These datasets are growing, and becoming
            more user-friendly, making them accessible to anyone. SkipAtom vectors can also be extended, by taking into
            consideration other aspects of chemistry, such as the oxidation states of atoms in a material (imagine
            learning representations for atoms in their various oxidation states). We envision researchers developing
            pre-trained SkipAtom vectors for their own, custom datasets. While the results aren't shown here, it appears
            that using pre-trained distributed representations of atoms helps most on tasks with smaller datasets. As
            new materials are sought to address pressing economic and environmental demands, smaller exploratory
            datasets of materials properties will likely be common. We expect SkipAtom vectors to play a role in
            devising effective models based on these smaller, targeted datasets.
        </p>

        <p>
            We've made the <a href="https://github.com/lantunes/skipatom" target="_blank">source code</a> freely
            available, distributed under the MIT license. There are trained SkipAtom atom vectors located in
            <a href="https://github.com/lantunes/skipatom/blob/main/data/skipatom_20201009_induced.csv" target="_blank">this file</a>,
            for easy incorporation into any codebase. We hope that this research will be of use to the Computational
            Materials Science community, and aid in the quest to discover new materials that address social, economic
            and environmental needs.
        </p>

        <hr style="border-top: 1px"/>

        <span class="post-notes">Notes</span>
        <ul class="notestext">
            <li>
                Read the <a href="https://www.nature.com/articles/s41524-022-00729-3" target="_blank">paper</a>
            </li>
            <li>
                Get the <a href="https://github.com/lantunes/skipatom" target="_blank">open source</a> code
            </li>
        </ul>

    </div>

    <!-- end content -->

    <div class="backbutton">
        <a href="/"><img src="images/left-arrow.png"></a>
    </div>

</div>

<footer>
    <div class="mediaicons">
        <div style="padding-right: 15px">
            <a href="mailto:info@materialis.ai"><img width="30px" src="images/email.png"></a>
        </div>
        <div>
            <a href="https://github.com/MaterialisAI"><img width="25px" src="images/github.png"></a>
        </div>
    </div>
    <p class="copyright">&copy; Materialis.AI 2022 | <a class="termslink" style="color:#000000" href="https://materialis.ai/terms.html">Terms &amp; Conditions</a></p>
</footer>

</body>
</html>
